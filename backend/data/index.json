[
  {
    "file_name": "projectss.pdf",
    "page": 1,
    "text": "Constructure AI - Applied LLM Engineer Technical Assignment (36-Hour \nChallenge) \nYou have 36 hours to submit this. \nRole and Tech Constraints \nThis assignment is for an Applied LLM / AI Engineer role. \nYou will be evaluated on how you: \n• Design retrieval and generation workflows \n• Handle messy, semi-structured construction data \n• Build something testable, not just a notebook demo \nRequired tech stack: \n• Backend: FastAPI \n• Frontend: React or Next.js \n• AI: Any LLM provider (for example: OpenAI, Anthropic, local model, etc.) \nYou are free to structure the system however you want within these constraints. \n \nHigh-Level Goal \nBuild a mini “Project Brain” for a single construction project: \n• Ingest a small set of project documents (PDFs and schedules we will provide). \n• Expose a chat interface where a user can: \no Ask natural language questions about the project. \no Request structured information (for example: “give me a door schedule”). \no Return source-backed answers and structured outputs using an LLM \npowered pipeline. \nThis should feel like a tiny, opinionated version of Constructure AI. \n "
  },
  {
    "file_name": "projectss.pdf",
    "page": 2,
    "text": "Part 0 - Deployment and Testability (Required) \nWe must be able to use what you build. \nRequirements: \n• Deploy the frontend to Vercel (free tier) and include a live URL in your README, \nsuch as: \n https://your-app-name.vercel.app \n• The frontend should talk to your FastAPI backend (cloud, tunnel, or Vercel-\ncompatible API). \n• We should be able to: \no Open the URL \no Interact with the chat \no Run queries against the sample project documents \nYour README should clearly explain: \n• How to run the backend locally \n• What environment variables are needed (keys, model names, etc.) \nNo need to over-engineer infrastructure, but this should feel like a real app, not just a \nscript. \n \nPart 1 - Document Ingestion and Indexing \nYou will be given a small set of construction project files (for example: specs, a drawing \nexport, and a schedule PDF). \nYour system should: \n• Ingest multiple documents for a single project. \n• Split them into searchable units (pages, sections, or chunks) using heuristics of \nyour choice. \n• Store them in some type of searchable index, including: \no Text content \no File name \no Page or section reference \no Any other metadata you find useful "
  },
  {
    "file_name": "projectss.pdf",
    "page": 3,
    "text": "You can use any combination of: \n• Vector search \n• Keyword search \n• Hybrid retrieval \nWe will not tell you how to chunk, embed, or store data — that is part of what we are \nevaluating. \n \nPart 2 - Project Q and A Chat (RAG) \nBuild a simple chat interface on your React or Next frontend that talks to your FastAPI \nbackend. \nThe chat should support natural language questions such as: \n• “What is the fire rating for corridor partitions?” \n• “What is the specified flooring material in the lobby?” \n• “Are there any accessibility requirements for doors?” \nFor each answer: \n• Use your retrieval pipeline to fetch relevant text from the ingested documents. \n• Use an LLM to generate a concise answer. \n• Return citations: file name plus page or section for the sources used. \n• Show the full conversation on the frontend (user and AI messages). \nWe are looking for: \n• Reasonable retrieval quality \n• Sensible prompting and answer formatting \n• Clear links back to sources (no hallucinations without caveats) \n \nPart 3 - Structured Extraction Task (Door or Element Schedule) \nThis is where it becomes closer to Constructure. \nImplement at least one structured extraction flow, for example: "
  },
  {
    "file_name": "projectss.pdf",
    "page": 4,
    "text": "• Door schedule extraction \n• Room summary extraction \n• MEP equipment list extraction \n(depends on the provided documents) \nRequirements: \nThe user should be able to trigger this through the chat, for example: \n• “Generate a door schedule” \n• “List all rooms with their area and floor finish” \nYour backend should: \n• Retrieve relevant parts of the documents. \n• Use an LLM (and any logic you design) to extract a structured JSON result. \nExample shape (for a door schedule): \n[ \n  { \n    \"mark\": \"D-101\", \n    \"location\": \"Level 1 Corridor\", \n    \"width_mm\": 900, \n    \"height_mm\": 2100, \n    \"fire_rating\": \"1 HR\", \n    \"material\": \"Hollow Metal\" \n  } \n] \n \nYour frontend should: \n• Display the structured result (for example: a table). \n• Still, show which pages or sections the data came from. \nYou are free to design the exact schema, but it should be: \n• Consistent \n• Reasonably complete given the data \n• Not obviously overfitted to one example "
  },
  {
    "file_name": "projectss.pdf",
    "page": 5,
    "text": " \nPart 4 - Minimal Evaluation and Introspection \nWe want to see how you think about quality, not only the system’s basic functionality. \nImplement a lightweight evaluation or introspection step, for example: \n• A small set of hard-coded test queries (5 to 10) with expected rough answers. \n• A simple script or endpoint that: \no Runs these queries through your pipeline \no Logs the model answers and their sources \no Outputs something like “looks correct / partially correct / wrong” based on \nsimple heuristics or manual labels \nWe are not expecting a full evaluation system just enough to show that you are thinking \nabout: \n• Failure modes \n• Retrieval errors \n• Model hallucinations \n \nBonus (Totally Optional but Very Impressive) \nIf you have time within the 36 hours, any of these will stand out: \nMulti-strategy Retrieval \n• Combine vector and keyword search and rerank results before sending to the \nLLM. \nPer-Tool Behavior \nDifferent “modes” or “tools” in the chat: \n• Q and A mode \n• Schedule extraction mode \n• “Show me sources only” mode \nCaching "
  },
  {
    "file_name": "projectss.pdf",
    "page": 6,
    "text": "• Cache retrieval results or LLM responses to speed up repeated queries. \nBasic Analytics \n• Log the types of questions asked and the documents most frequently used. \nRed-Flag Detection \nAdd a command such as: \n• “Are there any conflicting specs about door fire ratings?” \nTry to detect inconsistencies using retrieval plus LLM. \nThese are intentionally non-trivial only build them if your core flows are solid. \n \nSubmission \nYour public GitHub repo should include: \nREADME with: \n• Short description of your solution \n• How to run the FastAPI backend locally \n• How to run the React or Next frontend locally \n• Required environment variables (for example: OPENAI_API_KEY) \n• Deployed Vercel URL \n• Brief notes on: \no How you chunk and index the documents \no How your RAG pipeline works \no What you chose for the structured extraction task \no Any simple scripts or tools used for ingestion or evaluation \n \nHow We Evaluate \nWe will look at: \nCore functionality "
  },
  {
    "file_name": "projectss.pdf",
    "page": 7,
    "text": "• Can we log in with testingcheckuser1234@gmail.com  \n• Can we load your app, ask real questions about the project documents, and get \nusable answers with sources? \n• Does your structured extraction produce sensible structured data? \nAI / RAG design \n• Chunking, retrieval, prompting, error handling \n• How you handle noisy or partial documents \nCode quality \n• FastAPI and React or Next organization \n• Clear separation between data, retrieval, and LLM logic \nProduct sense \n• Does this feel like a plausible small slice of Constructure AI, not just a toy? \nDepth \n• How far you pushed within the 36-hour window \n• Whether you attempted any introspection or bonus features meaningfully \n "
  }
]